{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d06c8a-d762-46ae-a2ae-5e858bcbde80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1976f24-2909-4b38-b23f-1d17dadfd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering\n",
      "This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can somet\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1c1737-fce4-4785-a31f-18937f6e0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c59f4a39-d2d8-42dd-9ea7-56a693e37dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" {text}\n",
    "-------------------------------\n",
    "\n",
    "Based on the above instruction, help me write a good prompt TEMPLATE.\n",
    "\n",
    "This template should be a python f-string. It can take in any number of variables depending on my objective.\n",
    "\n",
    "Return your answer in the following format:\n",
    "\n",
    "```prompt\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "This is my objective:\n",
    "\n",
    "{objective}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d442d79-3c98-462f-a681-832772494fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(text=text)\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9638effd-e9bc-4a5a-b499-f287fc87e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is a Python f-string template that you can use to create prompts for a language model to answer questions based solely on the provided context. The template includes placeholders for the context (`context`) and the question (`question`) that you will need to fill in when using the template.\n",
      "\n",
      "```python\n",
      "prompt = f\"\"\"\n",
      "SYSTEM\n",
      "You will be provided with text delimited by triple quotes that contains all the information needed to answer the following question. Do not use any external knowledge or make assumptions beyond what is provided in the text. If the answer cannot be found in the text, respond with 'The information provided does not contain the answer to this question.'\n",
      "\n",
      "USER\n",
      "\\\"\\\"\\\"{{context}}\\\"\\\"\\\"\n",
      "\n",
      "Question: {{question}}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "When you want to use this template, you will replace `{context}` with the actual context text and `{question}` with the actual question you want the model to answer. Here's an example of how you might use the template:\n",
      "\n",
      "```python\n",
      "context = \"The quick brown fox jumps over the lazy dog.\"\n",
      "question = \"What does the fox do?\"\n",
      "\n",
      "filled_prompt = prompt.format(context=context, question=question)\n",
      "```\n",
      "\n",
      "This will produce a prompt that looks like this:\n",
      "\n",
      "```\n",
      "SYSTEM\n",
      "You will be provided with text delimited by triple quotes that contains all the information needed to answer the following question. Do not use any external knowledge or make assumptions beyond what is provided in the text. If the answer cannot be found in the text, respond with 'The information provided does not contain the answer to this question.'\n",
      "\n",
      "USER\n",
      "\"\"\"The quick brown fox jumps over the lazy dog.\"\"\"\n",
      "\n",
      "Question: What does the fox do?\n",
      "```\n",
      "\n",
      "You can then feed `filled_prompt` to the language model to get an answer based solely on the provided context."
     ]
    }
   ],
   "source": [
    "task = \"answer a question based on context provided, and ONLY on that context.\"\n",
    "\n",
    "for token in chain.invoke({\"objective\": task}):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575ab73-c841-4434-bc18-b72fc891f2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
